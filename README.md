# Example Quickstart:

> Have `llama-server` from `llama.cpp` running on port 8080.

```bash
# Create the environment
uv venv

# Install the dependencies
uv pip install -r requirements.txt

# Run the program
uv run python main.py
```

---

# âš ï¸ **Warning**: AI slop below:

# ðŸ¤– GPT-OSS: The Operatorâ€™s Console

Your terminal isnâ€™t just a shell â€” itâ€™s **the gateway**.

This console is a live, streaming interface into your **local** Large Language Model. No filler, no drag. Just signal. Just command.

Built for operators who bend systems, patch reality, and ship code at the speed of thought.

---

*(GIF Placeholder: Neon-lit console streaming character-by-character output; tool calls appear inline, then results fold in below.)*

## âœ¨ The Construct

- **Hyperâ€‘Stream** â€” Watch text and code materialize **characterâ€‘byâ€‘character** as tokens arrive. Model deltas render live; tool calls surface inline.
- **System Control** â€” Speak naturally, act in machine logic. Invoke `shell`, `read_file`, and `file_patch` without leaving the flow.
- **Visual Clarity** â€” A `rich` UI with panels and rules that keep long outputs readable.
- **Signal Integrity** â€” Clean, flickerâ€‘free streaming; tool results are **paneled** and long outputs are smartâ€‘truncated.
- **Architect Mode** â€” Extend with plain Python. Register your function and brief the model; the console does the rest.

## ðŸ”Œ Requirements

- A local model endpoint compatible with `llama.cpp` **chat completions** streaming.
- Default target: `http://localhost:8080/v1/chat/completions` (configurable in the code).

> **Tip:** Start `llama-server` from `llama.cpp` on port **8080** before running the console.

## ðŸš€ Jacking In

```bash
# Create the environment
uv venv

# Install dependencies
uv pip install -r requirements.txt

# Run the console
uv run python cli.py
````

## ðŸ’» Operating

Once inside, the system listens. It executes.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPT-OSS API CLI (Stable UI)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

You: Read the first 5 lines of tools.py, then run `python --version`.

Assistant: Accessing...
Calling Tool: read_file({"file_path": "tools.py", "end_line": 5})
Calling Tool: shell({"command": "python --version"})
```

After tools finish, the results are displayed in green panels. Long outputs are truncated with an inline â€œ(output truncated â€¦)â€ footer; full content is still passed back to the model.

## ðŸ§° The Arsenal (Builtâ€‘ins)

### `shell`

Execute any command your terminal understands. Output and errors are formatted into separate **STDOUT/STDERR** sections and time out safely.

**Example**

> You: what processes are running?
>
> Assistant: `shell("ps aux")`

### `read_file`

Read entire files or just a slice â€” with **1â€‘based** line numbers and an autoâ€‘generated header showing the window.

**Example**

> You: show me lines 1â€“40 of `cli.py`
>
> Assistant: `read_file("cli.py", start_line=1, end_line=40)`

### `file_patch`

Apply a **diffâ€‘style** patch inline â€” add (`+`), remove (`-`), or keep context (` `). Ideal for surgical edits without opening an editor.

**Example**

> You: in `cli.py`, change the API port from 8080 â†’ 9000
>
> Assistant:
>
> ```
> file_patch("cli.py", "-API_URL = \"http://localhost:8080/v1/chat/completions\"\n+API_URL = \"http://localhost:9000/v1/chat/completions\"")
> ```

**Patch rules (quick)**

* Start each changed line with `+` (add) or `-` (remove). Lines without a prefix are treated as context.
* File is rewritten from the original with your directives; keep enough context to avoid accidental deletions.

## ðŸ§© Becoming the Architect

Extend the console with your own tools in three steps.

1. **Forge the Tool** â€” add a Python function in `tools.py`:

```python
# tools.py
def get_system_load() -> str:
    import os
    l1, l5, l15 = os.getloadavg()
    return f"Load Average: {l1}, {l5}, {l15}"
```

2. **Register It** â€” add to `AVAILABLE_TOOLS`:

```python
AVAILABLE_TOOLS = {
    # ... existing tools
    "get_system_load": get_system_load,
}
```

3. **Brief the Model** â€” declare a tool schema in `cli.py` `tools_definition`:

```python
{
  "type": "function",
  "function": {
    "name": "get_system_load",
    "description": "Checks the current system load average.",
    "parameters": {"type": "object", "properties": {}}
  }
}
```

Thatâ€™s it. Your function is now invocable via tool calls, displayed inline as the stream arrives, and its result is fed back into the conversation.

## ðŸ”§ Tuning & UX Notes

* **Streaming:** Uses chunked lines (`data: {json}`) and prints **delta content** immediately.
* **Inline Tool Visualization:** The console prints `Calling Tool: name({...})` as arguments stream in, then panels the result.
* **Truncation:** Long tool outputs are truncated for readability (default **10 lines**) with an omittedâ€‘lines footer; adjust the limit in code.
* **Safety:** Shell commands have a 30â€‘second timeout and structured error reporting. File reads validate ranges and return friendly errors.

## ðŸ§ª Example Session

```text
You: Open README.md, show me 1â€“20, then count files in this directory.

Assistant: Accessing...
Calling Tool: read_file({"file_path": "README.md", "start_line": 1, "end_line": 20})
Calling Tool: shell({"command": "ls -1 | wc -l"})

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tool Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Tool Result: read_file]
### Showing lines 1-20 of `README.md` (Total: â€¦)
```

1: # Project
2: â€¦
...

```

[Tool Result: shell]
### Shell Command Successful
#### STDOUT
```

---

Welcome to the machine. Welcome to the real world.

